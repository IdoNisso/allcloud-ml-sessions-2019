{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `scikit-learn` Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use numpy to generate the data\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib for figures\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# The Scikit-learn package is called using 'sklearn'\n",
    "import sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we won't import sklearn like so as it's a big package. We should focus our imports to the specific things we need as we'll start doing below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification example\n",
    "### Generating data to work with\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "num_features = 3\n",
    "X = np.random.uniform(low=0.0, high=10.0, size=(num_samples,num_features))\n",
    "\n",
    "# Checking that the array is shaped as we wanted\n",
    "X.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set y to 1 for samples with an average of X's above 6\n",
    "y = np.zeros(num_samples).astype(int)\n",
    "y[X.mean(axis=1) > 6] = 1\n",
    "\n",
    "# Checking that we actually have 2 classes\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "dict(zip(unique, counts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our features are written in uppercase `X` and the labels are lowercase `y`. \n",
    "\n",
    "This is a naming convention, as `X` is a 2d array and `y` is a 1d array. If for some reason we generate a single sample (1d array), we should use `x`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test Split\n",
    "Sklearn can do a lot of heavy lifting for you, including splitting your data sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/validation/test split\n",
    "tmp_X_train, X_test, tmp_y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(tmp_X_train, tmp_y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_validation shape: ', X_validation.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_validation shape: ', y_validation.shape)\n",
    "print('y_test shape: ', y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model: kNN\n",
    "The process of choosing a model should never be done directly with the test set! Rather, we'll divide our data to train/validation/test and choose the best model using the validation set. This is done to avoid a phenomenon called *test set overfitting*, where the performance on the test set does not represent genralization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create the model object\n",
    "kNN_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the training data\n",
    "kNN_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = kNN_model.predict(X_validation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score our prediction\n",
    "Sklearn has a bunch of metrics you can use out of the box, check out the list [here](https://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('kNN baseline score: ', accuracy_score(y_validation, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, but let's try to make it better..\n",
    "\n",
    "### Finding a better model\n",
    "To find a better model, we'll need to test the performance of several algorithms on our toy dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Let's create some classifiers\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"Decision Tree\", \"Random Forest\", \"AdaBoost\",  \"Naive Bayes\"]\n",
    "classifiers = [KNeighborsClassifier(5),\n",
    "               SVC(kernel=\"linear\", C=0.025),\n",
    "               DecisionTreeClassifier(max_depth=5),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "               AdaBoostClassifier(),\n",
    "               GaussianNB()]\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train, y_train)                # sklearn has the same API for all algorithms:  .fit()\n",
    "    #clf.predict(X_test)                     # .predict()\n",
    "    val_score = clf.score(X_validation, y_validation)   # and even .score() which runs predict inside\n",
    "    print(name, '(validation):', val_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how our chosen model performs on the real test set..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel=\"linear\", C=0.025)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print('Linear SVM test score: ', model.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the performance on the test set is not identical to the performance on the validation set. This is expected.\n",
    "\n",
    "**Note:** If the difference between the validation and test performance is large, it might indicate we have problems!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "Don't be confused! Sklearn can help with confusion matrices!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# Plot the confusion matrix\n",
    "# Sklearn uses matplotlib behind the scenes\n",
    "plot_confusion_matrix(model, X_test, y_test);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also generate the textual version\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "conf_mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .. and visualize it however we want\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(conf_mat, annot=True);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression example\n",
    "As in the classifier example above, our steps are similar:\n",
    "1. Generate toy data / load real data\n",
    "2. Split data into train/validation/test\n",
    "3. Start with a simple baseline model\n",
    "4. Try to improve performance with additional algorithms\n",
    "5. Check test set performance with selected model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we'll use sklearn's built in datasets\n",
    "from sklearn.datasets import load_boston  # The boston house prices dataset\n",
    "\n",
    "# 1. Load data\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# 2. Split data\n",
    "tmp_X_train, X_test, tmp_y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(tmp_X_train, tmp_y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_validation shape: ', X_validation.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_validation shape: ', y_validation.shape)\n",
    "print('y_test shape: ', y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the y distribution (just so we know some background before we start modeling..)\n",
    "\n",
    "plt.figure(figsize=(6.5,6))\n",
    "plt.hist(y, alpha=0.7, color='firebrick', edgecolor='k', label='y (boston)')\n",
    "plt.xlabel('# samples')\n",
    "plt.ylabel('y value')\n",
    "plt.title(\"Distribution of y (boston)\")\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Baseline model\n",
    "from sklearn import linear_model\n",
    "\n",
    "reg_model = linear_model.LinearRegression()\n",
    "reg_model.fit(X_train, y_train)  # Same fit/predict API!\n",
    "\n",
    "print('Learned regression weights: \\n', reg_model.coef_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error (MSE) of baseline model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = reg_model.predict(X_validation)\n",
    "mse = mean_squared_error(y_validation, y_pred)\n",
    "\n",
    "print('Baseline Linear Regression MSE (validation): ', mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can improve this!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Find better algorithms\n",
    "\n",
    "names = [\"Linear Regression\", \"Ridge Regression\", \"Stochastic Gradient Descent Regression\", \"Bayesian Ridge Regression\"]\n",
    "regressors = [linear_model.LinearRegression(),\n",
    "              linear_model.Ridge(),\n",
    "              linear_model.SGDRegressor(),\n",
    "              linear_model.BayesianRidge()]\n",
    "\n",
    "for name, reg_model in zip(names, regressors):\n",
    "    reg_model.fit(X_train, y_train)\n",
    "    y_pred = reg_model.predict(X_validation)\n",
    "    mse = mean_squared_error(y_validation, y_pred)\n",
    "    print(name, 'MSE (validation):', mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our baseline plain-vanilla linear regression has comparable results to Ridge and Bayesian Ridge, with no real improvement. This can also happen :)\n",
    "\n",
    "In cases like these (similar performance) we'll want to choose the simplest model of the bunch, for easiest model interpertation.\n",
    "\n",
    "Let's see how it performs on the real test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_model = linear_model.LinearRegression()\n",
    "lin_reg_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lin_reg_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Linear Regression MSE (test):', mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
